{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data from the website.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://doj.gov.in/judges\"  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    judges_count_element = soup.find('div', class_='judges-count')  # Adjust the selector as needed\n",
    "    judges_count = judges_count_element.text\n",
    "    print(\"Number of judges in the Supreme Court:\", judges_count)\n",
    "else:\n",
    "    print(\"Failed to fetch data from the website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'About', 'description': 'Last updated: 02-08-2024', 'image_url': 'https://doj.gov.in/wp-content/themes/sdo-theme/images/search_icon.svg'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_doj_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract title\n",
    "    title = soup.find('h1').text\n",
    "\n",
    "    # Extract description\n",
    "    description = soup.find('p').text\n",
    "\n",
    "    # Extract image URL\n",
    "    image_url = soup.find('img')['src']\n",
    "\n",
    "    # Extract other information as needed\n",
    "    # ...\n",
    "\n",
    "    return {'title': title, 'description': description, 'image_url': image_url}\n",
    "\n",
    "# Replace 'https://doj.gov.in/about' with the actual URL you want to scrape\n",
    "url = 'https://doj.gov.in/about'\n",
    "result = scrape_doj_website(url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Tele-Law\n",
      "Description:  Download  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://doj.gov.in/tele-law/'  # Assuming this is the webpage URL\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and extract the relevant information\n",
    "title = soup.find('h1').get_text()  # Assuming the title is in an <h1> tag\n",
    "description = soup.find('p').get_text()  # Assuming the description is in a <p> tag\n",
    "\n",
    "print(\"Title:\", title)\n",
    "print(\"Description:\", description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ##Strarting complicated Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='web_scraping.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://doj.gov.in/'  # Example URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    logging.info(f'Successfully fetched the webpage: {url}')\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    logging.error(f'HTTP error occurred: {http_err}')\n",
    "except Exception as err:\n",
    "    logging.error(f'Other error occurred: {err}')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the title\n",
    "try:\n",
    "    title = soup.find('h1').get_text(strip=True)\n",
    "    logging.info(f'Extracted title: {title}')\n",
    "except AttributeError:\n",
    "    logging.error('Title not found on the page')\n",
    "\n",
    "# Extracting the description\n",
    "try:\n",
    "    description = soup.find('p').get_text(strip=True)\n",
    "    logging.info(f'Extracted description: {description}')\n",
    "except AttributeError:\n",
    "    logging.error('Description not found on the page')\n",
    "\n",
    "# Extracting specific sections, if any (e.g., under a specific div or section)\n",
    "try:\n",
    "    about_section = soup.find('div', class_='about-content')  # Example class name\n",
    "    if about_section:\n",
    "        content = about_section.get_text(separator='\\n', strip=True)\n",
    "        logging.info('Extracted about section content')\n",
    "    else:\n",
    "        logging.warning('About section not found')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error while extracting the about section: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Continue extracting information as shown before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the Edge WebDriver executable\n",
    "edge_driver_path = \"C://Users//Jayashrinidhi V//Downloads//edgedriver_win64//msedgedriver.exe\"\n",
    "\n",
    "# Configure Edge options (optional)\n",
    "edge_options = Options()\n",
    "edge_options.use_chromium = True  # Use the Chromium-based Edge browser\n",
    "\n",
    "# Initialize the Edge WebDriver\n",
    "service = EdgeService(executable_path=edge_driver_path)\n",
    "driver = webdriver.Edge(service=service, options=edge_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the webpage\n",
    "url = 'https://doj.gov.in/tele-law/'  # Replace with your target URL\n",
    "\n",
    "# Fetch the webpage\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Adjust the sleep time as needed to ensure the content loads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Tele-Law\n",
      "Description: Download\n"
     ]
    }
   ],
   "source": [
    "# Parse the page content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Example: Extracting the title\n",
    "try:\n",
    "    title = soup.find('h1').get_text(strip=True)\n",
    "    print(f'Title: {title}')\n",
    "except AttributeError:\n",
    "    logging.error('Title not found on the page')\n",
    "\n",
    "# Example: Extracting the description\n",
    "try:\n",
    "    description = soup.find('p').get_text(strip=True)\n",
    "    print(f'Description: {description}')\n",
    "except AttributeError:\n",
    "    logging.error('Description not found on the page')\n",
    "\n",
    "# Example: Extracting specific sections (e.g., under a specific div or section)\n",
    "try:\n",
    "    about_section = soup.find('div', class_='about-content')  # Example class name\n",
    "    if about_section:\n",
    "        content = about_section.get_text(separator='\\n', strip=True)\n",
    "        print(f'About Section Content: {content}')\n",
    "    else:\n",
    "        logging.warning('About section not found')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error while extracting the about section: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 tables on the page.\n",
      "Headers found in table 0: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n",
      "Headers found in table 1: []\n",
      "Headers found in table 2: ['', 'Particulars', 'Civil', 'Criminal', 'Total']\n",
      "Row data found: ['Pending Cases', '0 to 1 Years', '4221071(38.55%)', '11457209(34.27%)', '15678280(35.32%)']\n",
      "Row data found: ['Pending Cases', '1 to 3 Years', '2691700(24.58%)', '7760902(23.21%)', '10452602(23.55%)']\n",
      "Row data found: ['Pending Cases', '3 to 5 Years', '1460526(13.34%)', '4780876(14.3%)', '6241402(14.06%)']\n",
      "Row data found: ['Pending Cases', '5 to 10 Years', '1786442(16.31%)', '5973306(17.87%)', '7759748(17.48%)']\n",
      "Row data found: ['Pending Cases', '10 to 20 Years', '659977(6.03%)', '2956377(8.84%)', '3616354(8.15%)']\n",
      "Row data found: ['Pending Cases', '20 to 30 Years', '101830(1.21%)', '433035(1.21%)', '534865(1.21%)']\n",
      "Row data found: ['Pending Cases', 'Above 30 Years', '29360(0.27%)', '71435(0.21%)', '100795(0.23%)']\n",
      "Row data found: ['Pending Cases', 'Total', '10950906', '33433140', '44384046']\n",
      "Row data found: ['Case Type Wise', 'Original', '8196752', '30013382', '38210134']\n",
      "Row data found: ['Case Type Wise', 'Appeal', '480648', '412014', '892662']\n",
      "Row data found: ['Case Type Wise', 'Application', '659628', '2648228', '3307856']\n",
      "Row data found: ['Case Type Wise', 'Execution', '1398894', '68053', '1466947']\n",
      "Row data found: ['Stage Wise', 'Appearance/Service Related', '2594430', '15318865', '17913295']\n",
      "Row data found: ['Stage Wise', 'Compliance/Steps/stay', '1905766', '2116290', '4022056']\n",
      "Row data found: ['Stage Wise', 'Evidence/Argument/Judgement', '4392076', '10734813', '15126889']\n",
      "Row data found: ['Stage Wise', 'Pleadings/Issues/Charge', '1453146', '2388213', '3841359']\n",
      "Row data found: ['Institution', 'Cases Instituted in Last Month', '384295', '2189687', '2573982']\n",
      "Row data found: ['Disposal', 'Cases Disposed in Last Month', '396367', '2336563', '2732930']\n",
      "Row data found: ['Senior Citizen', 'Filed Cases By Senior Citizen', '2220231', '672944', '2893175']\n",
      "Row data found: ['Woman', 'Filed Cases By Woman', '1772283', '1907083', '3679366']\n",
      "Row data found: ['Delay Reason Wise', 'Delay Reason', '4208147', '15178378', '19386525']\n",
      "Headers found in table 3: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n",
      "Headers found in table 4: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n",
      "Headers found in table 5: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n",
      "Headers found in table 6: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n",
      "Headers found in table 7: []\n",
      "Row data found: ['']\n",
      "Empty row found, skipping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: '', 'Civil', 'Total', 'Criminal', 'Particulars'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Save the scraped data to a CSV file\u001b[39;00m\n\u001b[0;32m     59\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecourts_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 60\u001b[0m \u001b[43msave_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscraped_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraped data has been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m, in \u001b[0;36msave_to_csv\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m     51\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jayas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: '', 'Civil', 'Total', 'Criminal', 'Particulars'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the website to be scraped\n",
    "base_url = 'https://njdg.ecourts.gov.in/'\n",
    "\n",
    "# Send a request to fetch the HTML content of the main page\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to scrape table data\n",
    "def scrape_table_data(soup):\n",
    "    data = []\n",
    "    \n",
    "    # Find all tables in the HTML\n",
    "    tables = soup.find_all('table')\n",
    "    print(f\"Found {len(tables)} tables on the page.\")  # Debugging statement\n",
    "\n",
    "    for table_index, table in enumerate(tables):\n",
    "        headers = []\n",
    "        # Extract headers\n",
    "        header_row = table.find('tr')\n",
    "        if header_row:\n",
    "            headers = [header.get_text(strip=True) for header in header_row.find_all('th')]\n",
    "            print(f\"Headers found in table {table_index}: {headers}\")  # Debugging statement\n",
    "        else:\n",
    "            print(f\"No headers found in table {table_index}.\")  # Debugging statement\n",
    "        \n",
    "        # Extract rows of data\n",
    "        for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cols = row.find_all('td')\n",
    "            cols = [col.get_text(strip=True) for col in cols]\n",
    "            if cols:\n",
    "                print(f\"Row data found: {cols}\")  # Debugging statement\n",
    "                data.append(dict(zip(headers, cols)))\n",
    "            else:\n",
    "                print(\"Empty row found, skipping.\")  # Debugging statement\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    \n",
    "    headers = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Scrape table data from the page\n",
    "scraped_data = scrape_table_data(soup)\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "csv_file_path = 'ecourts_data.csv'\n",
    "save_to_csv(scraped_data, csv_file_path)\n",
    "\n",
    "print(f\"Scraped data has been saved to {csv_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
