{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-dict item: text_content\n",
      "Skipping non-dict item: links\n",
      "Skipping non-dict item: headers\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('C://Users//Jayashrinidhi V//OneDrive//Documents//VScode//Justice_bot//doj_website_data.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize lists for training data\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "\n",
    "# Process the JSON data\n",
    "for item in data:\n",
    "    if isinstance(item, dict):  # Check if the item is a dictionary\n",
    "        # Extract fields if they exist\n",
    "        page_content = item.get('page_content', '')\n",
    "        if page_content:\n",
    "            training_sentences.append(page_content)\n",
    "            training_labels.append('page_content')  # Label for the content\n",
    "\n",
    "        link_text = item.get('link_text', '')\n",
    "        if link_text:\n",
    "            training_sentences.append(link_text)\n",
    "            training_labels.append('link_text')  # Label for the link text\n",
    "\n",
    "        url = item.get('url', '')\n",
    "        if url:\n",
    "            training_sentences.append(url)\n",
    "            training_labels.append('url')  # Label for the URL\n",
    "    else:\n",
    "        print(f\"Skipping non-dict item: {item}\")\n",
    "\n",
    "# Continue with the rest of your pipeline...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('C://Users//Jayashrinidhi V//OneDrive//Documents//VScode//Justice_bot//doj_full_scrape.json') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'link_text': '\\nSkip to main content\\n\\n', 'url': 'https://doj.gov.in/#SkipContent', 'page_content': 'As per the Allocation of Business (Rules), 1961, Department of Justice is a part of Ministry of Law & Justice, Government of India. It is one of the oldest Ministries of the Government of India. Till 31.12.2009, Department of Justice was part of Ministry of Home Affairs and Union Home Secretary had been the Secretary of Department of Justice. Keeping in view the increasing workload and formulating many policies and programmes on Judicial Reforms in the country, a separate Department namely Department of Justice was carved out from MHA and placed under the charge of Secretary to Government of India and it started working as such from 1st January, 2010 under the Ministry of Law & Justice. The Department is housed in the Jaisalmer House, 26, Man Singh Road, New Delhi. The Organizational setup of the Department includes 04 Joint Secretaries, 08 Directors/ Deputy Secretaries and 09 Under Secretaries. The functions of the Department of Justice include the appointment, resignation and removal of the Chief Justice of India, Judges of the Supreme Court of India, Chief Justices and Judges of the High Courts and their service matters. In addition, the Department implements important schemes for Development of Infrastructure Facilities for Judiciary, setting up of Special Courts for speedy trial and disposal of cases of sensitive nature (Fast Track Special Court for cases of rape and POCSO Act), E-court Project on computerization of various courts across the country, legal aid to poor and access to justice, financial assistance to National Judicial Academy for providing training to the Judicial Officers of the country. The functions of Department of Justice are given in Allocation of Business (Rules), 1961\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                Content Owned by                                DEPARTMENT OF JUSTICE, Ministry of Law and Justice, GOI                            \\nDeveloped and hosted by National Informatics Centre, Ministry of Electronics & Information Technology, Government of India\\nLast Updated: Aug 22, 2024'}, {'link_text': '\\n\\n', 'url': 'https://doj.gov.in/javascript:void(0);', 'page_content': 'Something is wrong with your request (like the page your are looking for does not exist).\\nPlease try after some time or click here to go to Home Page.'}, {'link_text': '\\n\\nSocial Media Links\\n', 'url': 'https://doj.gov.in/javascript:void(0);', 'page_content': 'Something is wrong with your request (like the page your are looking for does not exist).\\nPlease try after some time or click here to go to Home Page.'}, {'link_text': '', 'url': 'https://www.facebook.com/MLJ.GovIndia', 'page_content': ''}, {'link_text': '', 'url': 'https://twitter.com/MLJ_GoI', 'page_content': ''}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('C://Users//Jayashrinidhi V//OneDrive//Documents//VScode//Justice_bot//doj_full_scrape.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the type of data\n",
    "print(type(data))  # This will tell you whether data is a dict, list, etc.\n",
    "\n",
    "# If it's a dictionary, print its keys\n",
    "if isinstance(data, dict):\n",
    "    print(data.keys())  # Print the keys to see the structure\n",
    "\n",
    "# If it's a list, you can then proceed with printing the first few items\n",
    "elif isinstance(data, list):\n",
    "    print(data[:5])  # Print the first 5 items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process the JSON data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Assuming each item in the list is a dictionary with 'link_text', 'url', 'page_content'\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     page_content \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page_content:\n\u001b[0;32m     22\u001b[0m         training_sentences\u001b[38;5;241m.\u001b[39mappend(page_content)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the JSON file\n",
    "with open('C://Users//Jayashrinidhi V//OneDrive//Documents//VScode//Justice_bot//doj_website_data.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize lists for training data\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "\n",
    "# Process the JSON data\n",
    "for item in data:\n",
    "    # Assuming each item in the list is a dictionary with 'link_text', 'url', 'page_content'\n",
    "    page_content = item.get('page_content', '')\n",
    "    if page_content:\n",
    "        training_sentences.append(page_content)\n",
    "        training_labels.append('page_content')  # Label for the content\n",
    "    \n",
    "    # You can also include 'link_text' and 'url' if needed\n",
    "    link_text = item.get('link_text', '')\n",
    "    if link_text:\n",
    "        training_sentences.append(link_text)\n",
    "        training_labels.append('link_text')  # Label for the link text\n",
    "    \n",
    "    url = item.get('url', '')\n",
    "    if url:\n",
    "        training_sentences.append(url)\n",
    "        training_labels.append('url')  # Label for the URL\n",
    "\n",
    "# Update labels list if using label encoding\n",
    "for label in ['page_content', 'link_text', 'url']:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "training_labels = label_encoder.fit_transform(training_labels)\n",
    "\n",
    "# Tokenize the sentences\n",
    "vocab_size = 1000  # Adjust as necessary\n",
    "max_length = 200  # Adjust based on your data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to categorical format if using a neural network\n",
    "training_labels = to_categorical(training_labels, num_classes=len(labels))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, training_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can proceed with building, training, and evaluating your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense, Dropout\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m----> 6\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_length),\n\u001b[0;32m      7\u001b[0m     LSTM(\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      8\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      9\u001b[0m     LSTM(\u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     10\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     11\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     12\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(labels), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10,          # Adjust epochs based on your needs\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,      # Adjust batch size as necessary\n",
    "    verbose=2           # Display training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to label format\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Print some sample predictions\n",
    "for i in range(5):\n",
    "    print(f'Input: {X_test[i]}')\n",
    "    print(f'Predicted Label: {predicted_labels[i]}')\n",
    "    print(f'Actual Label: {label_encoder.inverse_transform([y_test[i].argmax()])[0]}')\n",
    "    print('---')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
